{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = 16, 8\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-dark')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import gym\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Final\n",
    "\n",
    "Curso Aprendizaje por Refuerzos, Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones\n",
    "\n",
    "FaMAF, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "En el siguiente notebook se muestra cómo ejecutar agentes de aprendizaje por refuerzos que hacen uso de modelos matemáticos para aproximar una función de valor. Al utilizar este tipo de herramientas, aumentamos el riesgo de ver un comportamiento inestable y divergente durante el entrenamiento del agente. Esto se da por la presencia de tres aspectos principales, llamados 'Triada Mortal':\n",
    "\n",
    "* Aproximación de funciones: es una forma poderosa y escalable de generalizar desde un espacio de estado mucho más grande que la memoria y los recursos computacionales (por ejemplo, aproximación de función lineal o Redes Neuronales Artificiales).\n",
    "\n",
    "* Bootstrapping: se actualizan objetivos que son dependientes de las estimaciones existentes (por ejemplo, en los métodos de TD) en lugar de depender exclusivamente de las recompensas reales (como en aprendizaje supervisado).\n",
    "\n",
    "* Off-Policy: Entrenamiento de un agente en base a una política diferente a la política objetivo (Por ejemplo Q-Learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole: Agente aleatorio\n",
    "CartPole es un entorno donde un poste está unido por una unión no accionada a un carro, que se mueve a lo largo de una pista sin fricción. El sistema se controla aplicando una fuerza de +1 o -1 al carro. El péndulo comienza en posición vertical, y el objetivo es evitar que se caiga. Se proporciona una recompensa de +1 por cada paso de tiempo que el poste permanezca en posición vertical. El episodio termina cuando el poste está a más de 15 grados de la vertical, o el carro se mueve más de 2.4 unidades desde el centro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(250):\n",
    "    env.render(mode='human')\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # se ejecuta una acción aleatoria\n",
    "    if done:\n",
    "        env.reset()\n",
    "env.close()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole: Aproximación con un modelo lineal\n",
    "A continuación analizaremos la implementación de un agente Q-Learning que utiliza como aproximador de la función de valor, un modelo lineal.\n",
    "\n",
    "Un gran problema en el uso de aproximadores para la función de valor $Q$, es que las transiciones (la experiencia del agente) están muy correlacionadas. Esto reduce la varianza general de cada transición y disminuye la capacidad de generalizacion del predictor. \n",
    "Imaginemos que tuviéramos que aprender una tarea sin memoria (ni siquiera a corto plazo), siempre optimizaría el aprendizaje en función del último episodio.\n",
    "\n",
    "Para solucionar este problema, el equipo de investigación de Google DeepMind utilizó lo que llamaron 'Experience Replay' para minimizar este problema. Esto es, en lugar de actualizar la función de valor con la última transición realizada, se almacena la misma en una memoria de experiencia y después de cada interacción del agente con el entorno, se entrena la función de valor con un lote de transiciones muestreadas al azar de la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-aaab7693ac1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mLinearModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_input_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_output_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLinearModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_input_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_output_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class LinearModel(nn.Module):\n",
    "\n",
    "    def __init__(self, _input_size: int, _output_size: int):\n",
    "        super(LinearModel, self).__init__()       \n",
    "        self.output = nn.Linear(_input_size, _output_size)\n",
    "\n",
    "        # init weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from agents.utils.memory.ReplayMemory import ReplayMemory\n",
    "from agents.utils.memory.Transition import Transition\n",
    "\n",
    "class LinearCartPoleSolver:\n",
    "    def __init__(self, env, n_episodes=3000, max_env_steps=None, gamma=0.9,\n",
    "                 epsilon=0.5, epsilon_min=0.01, epsilon_log_decay=0.005, alpha=1e-3,\n",
    "                 memory_size=10000, batch_size=256, render=False, debug=False):\n",
    "\n",
    "        self.memory = ReplayMemory(capacity=memory_size)\n",
    "        self.env = env\n",
    "\n",
    "        # hyper-parameter setting\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.n_episodes = n_episodes\n",
    "        self.batch_size = batch_size\n",
    "        if max_env_steps is not None:\n",
    "            self.env._max_episode_steps = max_env_steps\n",
    "        self.observation_space_size = env.observation_space.shape[0]\n",
    "        self.action_space_size = env.action_space.n\n",
    "\n",
    "        self.render = render\n",
    "        self.debug = debug\n",
    "        if debug:\n",
    "            self.loss_list = []\n",
    "        # if gpu is to be used\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Init model 1\n",
    "        # Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "        # is a Module which contains other Modules, and applies them in sequence to\n",
    "        # produce its output. Each Linear Module computes output from input using a\n",
    "        # linear function, and holds internal Tensors for its weight and bias.\n",
    "        # After constructing the model we use the .to() method to move it to the\n",
    "        # desired device.\n",
    "        self.model = LinearModel(self.observation_space_size, self.action_space_size).to(self.device)        \n",
    "        self.model.train()\n",
    "\n",
    "        # The nn package also contains definitions of popular loss functions; in this\n",
    "        # case we will use Mean Squared Error (MSE) as our loss function. Setting\n",
    "        # reduction='sum' means that we are computing the *sum* of squared errors rather\n",
    "        # than the mean; this is for consistency with the examples above where we\n",
    "        # manually compute the loss, but in practice it is more common to use mean\n",
    "        # squared error as a loss by setting reduction='elementwise_mean'.\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        # Use the optim package to define an Optimizer that will update the weights of\n",
    "        # the model for us. Here we will use Adam; the optim package contains many other\n",
    "        # optimization algoriths. The first argument to the Adam constructor tells the\n",
    "        # optimizer which Tensors it should update.\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=alpha)\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        \"\"\"Chooses the next action according to the model trained and the policy\"\"\"\n",
    "\n",
    "        # exploits the current knowledge if the random number > epsilon, otherwise explores\n",
    "        if np.random.random() <= epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q = self.model(state)\n",
    "                argmax = torch.argmax(q)\n",
    "                return argmax.item()\n",
    "\n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"Returns an epsilon that decays over time until a minimum epsilon value is reached; in this case the minimum\n",
    "        value is returned\"\"\"\n",
    "        return max(self.epsilon_min, self.epsilon * math.exp(-self.epsilon_decay * episode))\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Previously stored (s, a, r, s') tuples are replayed (that is, are added into the model). The size of the\n",
    "        tuples added is determined by the batch_size parameter\"\"\"\n",
    "\n",
    "        transitions, _ = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        non_final_mask = torch.stack(batch.done)\n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.stack(batch.action)\n",
    "        reward_batch = torch.stack(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        with torch.no_grad():\n",
    "            next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "            next_state_values[non_final_mask] = self.model(non_final_next_states).max(1)[0].detach()\n",
    "            # Compute the expected Q values\n",
    "            expected_state_action_values = reward_batch + self.gamma * next_state_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        if self.debug:\n",
    "            self.loss_list.append(loss)\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main loop that controls the execution of the agent\"\"\"\n",
    "\n",
    "        scores = []\n",
    "        mean_scores = []\n",
    "        for e in range(self.n_episodes):\n",
    "            state = self.env.reset()\n",
    "            state = torch.tensor(state, device=self.device, dtype=torch.float)\n",
    "            done = False\n",
    "            cum_reward = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(\n",
    "                    state,\n",
    "                    self.get_epsilon(e))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = torch.tensor(next_state, device=self.device, dtype=torch.float)\n",
    "\n",
    "                cum_reward += reward\n",
    "                self.memory.push(\n",
    "                    state,  #Converted to tensor before\n",
    "                    torch.tensor([action], device=self.device),\n",
    "                    None if done else next_state,\n",
    "                    torch.tensor(reward, device=self.device).clamp_(-1, 1),\n",
    "                    torch.tensor(not done, device=self.device, dtype=torch.bool))\n",
    "\n",
    "                if self.memory.__len__() >= self.batch_size:\n",
    "                    self.replay()\n",
    "                state = next_state\n",
    "\n",
    "            scores.append(cum_reward)\n",
    "            mean_score = np.mean(scores)\n",
    "            mean_scores.append(mean_score)\n",
    "            if e % 100 == 0 and self.debug:\n",
    "                print('[Episode {}] - Mean reward {}.'.format(e, mean_score))\n",
    "\n",
    "        # noinspection PyUnboundLocalVariable\n",
    "        print('[Episode {}] - Mean reward {}.'.format(e, mean_score))\n",
    "        return scores, mean_scores\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = LinearCartPoleSolver(gym.make('CartPole-v0'), n_episodes=5000, debug=True)\n",
    "scoresLineal, meanLineal = agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se muestra el reward/score optenido por episodio\n",
    "plt.plot(np.array(scoresLineal), label='Lineal', c='#5c8cbc')\n",
    "plt.ylim(0, 200)\n",
    "plt.title('Recompensa por episodio')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(meanLineal), label='Lineal', c='#5c8cbc')\n",
    "plt.ylim(0, 200)\n",
    "plt.title('Recompensa promedio por episodio')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole: Aproximación con un modelo lineal con 'feature construction'\n",
    "Supongamos que un problema de aprendizaje por refuerzo que tiene estados con dos dimensiones numéricas. Para un estado $s$, sus dos componentes son $s_1 \\in R$ y $s_2 \\in R$. Se puede elegir representar $s$ simplemente por las dos dimensiones del estado, de modo que $x(s)=(s_1, s_2)$, pero así, no se puede tener en cuenta ninguna interacción entre estas dimensiones. Además, si tanto $s_1$ como $s_2$ son cero, entonces el valor aproximado también será cero ($x(s)=0$). Ambas limitaciones se pueden superar representando $s$ por el vector de features polinomico $x(s)=(1,s_1,s_2,s_1s_2)$. El 1 inicial, permite la representación de funciones afines en los números de estado originales, y el feature del producto final, $s1s2$, permite que se tengan en cuenta dichas interacciones. También se puede utilizar un vector de features polinomico de mayor orden, como $x(s)=(1,s_1,s_2,s_1s_2,s_1^2,s_2^2,s_1s_2^2,s_1^2s_2,s_1^2s_2^2)$, para poder modelar interacciones mas complejas.\n",
    "\n",
    "En el caso del entorno CartPole, un predictor lineal que tenga como input unicamente las variables que el entorno emite no puede modelar la relacion entre la posición y velocidad del carro con respecto al angulo y velocidad del poste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyCartPoleSolver:\n",
    "    def __init__(self, env, n_episodes=3000, max_env_steps=None, gamma=0.9,\n",
    "                 epsilon=0.5, epsilon_min=0.01, epsilon_log_decay=0.005, alpha=1e-3,\n",
    "                 memory_size=10000, batch_size=256, render=False, debug=False):\n",
    "\n",
    "        self.memory = ReplayMemory(capacity=memory_size)\n",
    "        self.env = env\n",
    "\n",
    "        # hyper-parameter setting\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.n_episodes = n_episodes\n",
    "        self.batch_size = batch_size\n",
    "        if max_env_steps is not None:\n",
    "            self.env._max_episode_steps = max_env_steps\n",
    "        self.observation_space_size = env.observation_space.shape[0]\n",
    "        self.action_space_size = env.action_space.n\n",
    "\n",
    "        self.feature_tuning = PolynomialFeatures(interaction_only=True)\n",
    "        # Initialize feature tunning\n",
    "        self.feature_tuning.fit(self.env.reset().reshape((1, 4)))\n",
    "\n",
    "        self.render = render\n",
    "        self.debug = debug\n",
    "        if debug:\n",
    "            self.loss_list = []\n",
    "        # if gpu is to be used\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Init model 1\n",
    "        # Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "        # is a Module which contains other Modules, and applies them in sequence to\n",
    "        # produce its output. Each Linear Module computes output from input using a\n",
    "        # linear function, and holds internal Tensors for its weight and bias.\n",
    "        # After constructing the model we use the .to() method to move it to the\n",
    "        # desired device.\n",
    "        self.model = LinearModel(self.feature_tuning.n_output_features_, self.action_space_size).to(self.device)\n",
    "        self.model.train()\n",
    "\n",
    "        # The nn package also contains definitions of popular loss functions; in this\n",
    "        # case we will use Mean Squared Error (MSE) as our loss function. Setting\n",
    "        # reduction='sum' means that we are computing the *sum* of squared errors rather\n",
    "        # than the mean; this is for consistency with the examples above where we\n",
    "        # manually compute the loss, but in practice it is more common to use mean\n",
    "        # squared error as a loss by setting reduction='elementwise_mean'.\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        # Use the optim package to define an Optimizer that will update the weights of\n",
    "        # the model for us. Here we will use Adam; the optim package contains many other\n",
    "        # optimization algoriths. The first argument to the Adam constructor tells the\n",
    "        # optimizer which Tensors it should update.\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=alpha)\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        \"\"\"State and action are stacked horizontally and its features are combined as a polynomial to be passed as an\n",
    "        input of the approximator\"\"\"\n",
    "\n",
    "        # poly_state converts the horizontal stack into a combination of its parameters i.e.\n",
    "        # [1, s_1, s_2, s_3, s_4, a_1, s_1 s_2, s_1 s_3, ...]\n",
    "        poly_state = self.feature_tuning.transform(state.reshape((1, 4)))\n",
    "        return poly_state[0]\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        \"\"\"Chooses the next action according to the model trained and the policy\"\"\"\n",
    "\n",
    "        # exploits the current knowledge if the random number > epsilon, otherwise explores\n",
    "        if np.random.random() <= epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q = self.model(state)\n",
    "                argmax = torch.argmax(q)\n",
    "                return argmax.item()\n",
    "\n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"Returns an epsilon that decays over time until a minimum epsilon value is reached; in this case the minimum\n",
    "        value is returned\"\"\"\n",
    "        return max(self.epsilon_min, self.epsilon * math.exp(-self.epsilon_decay * episode))\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Previously stored (s, a, r, s') tuples are replayed (that is, are added into the model). The size of the\n",
    "        tuples added is determined by the batch_size parameter\"\"\"\n",
    "\n",
    "        transitions, _ = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        non_final_mask = torch.stack(batch.done)\n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.stack(batch.action)\n",
    "        reward_batch = torch.stack(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        with torch.no_grad():\n",
    "            next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "            next_state_values[non_final_mask] = self.model(non_final_next_states).max(1)[0].detach()\n",
    "            # Compute the expected Q values\n",
    "            expected_state_action_values = reward_batch + self.gamma * next_state_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        if self.debug:\n",
    "            self.loss_list.append(loss)\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main loop that controls the execution of the agent\"\"\"\n",
    "\n",
    "        scores = []\n",
    "        mean_scores = []\n",
    "        for e in range(self.n_episodes):\n",
    "            state = self.env.reset()\n",
    "            state = torch.tensor(self.preprocess_state(state), device=self.device, dtype=torch.float)\n",
    "            done = False\n",
    "            cum_reward = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(\n",
    "                    state,\n",
    "                    self.get_epsilon(e))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = torch.tensor(self.preprocess_state(next_state), device=self.device, dtype=torch.float)\n",
    "\n",
    "                cum_reward += reward\n",
    "                self.memory.push(\n",
    "                    state,  # Converted to tensor before\n",
    "                    torch.tensor([action], device=self.device),\n",
    "                    None if done else next_state,\n",
    "                    torch.tensor(reward, device=self.device).clamp_(-1, 1),\n",
    "                    torch.tensor(not done, device=self.device, dtype=torch.bool))\n",
    "\n",
    "                if self.memory.__len__() >= self.batch_size:\n",
    "                    self.replay()\n",
    "                state = next_state\n",
    "\n",
    "            scores.append(cum_reward)\n",
    "            mean_score = np.mean(scores)\n",
    "            mean_scores.append(mean_score)\n",
    "            if e % 100 == 0 and self.debug:\n",
    "                print('[Episode {}] - Mean reward {}.'.format(e, mean_score))\n",
    "\n",
    "        # noinspection PyUnboundLocalVariable\n",
    "        print('[Episode {}] - Mean reward {}.'.format(e, mean_score))\n",
    "        return scores, mean_scores\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PolyCartPoleSolver(gym.make('CartPole-v0'), n_episodes=5000, debug=True)\n",
    "scoresPoly, meanPoly = agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se muestra el reward/score optenido por episodio\n",
    "plt.plot(np.array(scoresPoly), label='Poly', c='#faa76c')\n",
    "plt.plot(np.array(scoresLineal), label='Lineal', c='#5c8cbc')\n",
    "plt.ylim(0, 200)\n",
    "plt.title('Recompensa por episodio')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(meanPoly), label='Poly', c='#faa76c')\n",
    "plt.plot(np.array(meanLineal), label='Lineal', c='#5c8cbc')\n",
    "plt.ylim(0, 200)\n",
    "plt.title('Recompensa promedio por episodio')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole: Aproximación con un modelo lineal y 'Modelo Duplicado'\n",
    "El objetivo para una actualización de un algoritmo de control basado en TD, depende de la estimación actual ($w_{t}$). Donde el objetivo ($w_{t+1}$), es una función dependiente de los mismos parámetros que se están actualizando. Por ejemplo:\n",
    "\n",
    "$$w_{t+1}=w_t+\\alpha[R_{t+1}+\\gamma \\operatorname*{max}_a\\hat{q}(S_{t+1},a,w_t)-\\hat{q}(S_t,A_t,w_t)]\\nabla\\hat{q}(S_t,A_t,w_t)$$\n",
    "\n",
    "Esta dependencia de $w_{t}$ puede conducir a oscilaciones y/o divergencias en el entrenamiento del predictor.\n",
    "\n",
    "Para abordar este problema, el equipo de Google DeepMind desarrollo una solución para acercar el proceso de actualización de los parámetros ($w_t$) de un estimador, a un formato similar al utilizado en el aprendizaje supervisado. Esto es, se utiliza otro modelo predictivo $\\tilde{q}$ como objetivo en la actualización de los pesos $w$ del estimador $\\hat{q}$. Cada vez que se realizan un cierto número $C$ de actualizaciones de los pesos $w$ en el predictor $\\hat{q}$, se realiza una copia de los mismos en el otro modelo $\\tilde{q}$ donde se mantienen estos pesos duplicados de forma fija para las próximas $C$ actualizaciones de $w$.\n",
    "\n",
    "$$w_{t+1}=w_t+\\alpha[R_{t+1}+\\gamma \\operatorname*{max}_a\\underline{\\tilde{q}(S_{t+1},a,w_t)}-\\hat{q}(S_t,A_t,w_t)]\\nabla\\hat{q}(S_t,A_t,w_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyDualCartPoleSolver:\n",
    "    def __init__(self, env, n_episodes=3000, max_env_steps=None, gamma=0.9,\n",
    "                 epsilon=0.5, epsilon_min=0.01, epsilon_log_decay=0.005, alpha=1e-3,\n",
    "                 memory_size=10000, batch_size=256, c=10, render=False, debug=False):\n",
    "        self.memory = ReplayMemory(capacity=memory_size)\n",
    "        self.env = env\n",
    "\n",
    "        # hyper-parameter setting\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.n_episodes = n_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.c = c\n",
    "        if max_env_steps is not None:\n",
    "            self.env._max_episode_steps = max_env_steps\n",
    "        self.observation_space_size = env.observation_space.shape[0]\n",
    "        self.action_space_size = env.action_space.n\n",
    "\n",
    "        self.feature_tuning = PolynomialFeatures(interaction_only=True)\n",
    "        # Initialize feature tunning\n",
    "        self.feature_tuning.fit(self.env.reset().reshape((1, 4)))\n",
    "\n",
    "        self.render = render\n",
    "        self.debug = debug\n",
    "        if debug:\n",
    "            self.loss_list = []\n",
    "        # if gpu is to be used\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Init model 1\n",
    "        # Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "        # is a Module which contains other Modules, and applies them in sequence to\n",
    "        # produce its output. Each Linear Module computes output from input using a\n",
    "        # linear function, and holds internal Tensors for its weight and bias.\n",
    "        # After constructing the model we use the .to() method to move it to the\n",
    "        # desired device.\n",
    "        self.model = LinearModel(self.feature_tuning.n_output_features_, self.action_space_size).to(self.device)\n",
    "        self.model.train()\n",
    "        self.target = LinearModel(self.feature_tuning.n_output_features_, self.action_space_size).to(self.device)\n",
    "        self.target.load_state_dict(self.model.state_dict())\n",
    "        self.target.eval()\n",
    "\n",
    "        # The nn package also contains definitions of popular loss functions; in this\n",
    "        # case we will use Mean Squared Error (MSE) as our loss function. Setting\n",
    "        # reduction='sum' means that we are computing the *sum* of squared errors rather\n",
    "        # than the mean; this is for consistency with the examples above where we\n",
    "        # manually compute the loss, but in practice it is more common to use mean\n",
    "        # squared error as a loss by setting reduction='elementwise_mean'.\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        # Use the optim package to define an Optimizer that will update the weights of\n",
    "        # the model for us. Here we will use Adam; the optim package contains many other\n",
    "        # optimization algoriths. The first argument to the Adam constructor tells the\n",
    "        # optimizer which Tensors it should update.\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=alpha)\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        \"\"\"State and action are stacked horizontally and its features are combined as a polynomial to be passed as an\n",
    "        input of the approximator\"\"\"\n",
    "\n",
    "        # poly_state converts the horizontal stack into a combination of its parameters i.e.\n",
    "        # [1, s_1, s_2, s_3, s_4, a_1, s_1 s_2, s_1 s_3, ...]\n",
    "        poly_state = self.feature_tuning.transform(state.reshape((1, 4)))\n",
    "        return poly_state[0]\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        \"\"\"Chooses the next action according to the model trained and the policy\"\"\"\n",
    "\n",
    "        # exploits the current knowledge if the random number > epsilon, otherwise explores\n",
    "        if np.random.random() <= epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q = self.model(state)\n",
    "                argmax = torch.argmax(q)\n",
    "                return argmax.item()\n",
    "\n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"Returns an epsilon that decays over time until a minimum epsilon value is reached; in this case the minimum\n",
    "        value is returned\"\"\"\n",
    "        return max(self.epsilon_min, self.epsilon * math.exp(-self.epsilon_decay * episode))\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Previously stored (s, a, r, s') tuples are replayed (that is, are added into the model). The size of the\n",
    "        tuples added is determined by the batch_size parameter\"\"\"\n",
    "\n",
    "        transitions, _ = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        non_final_mask = torch.stack(batch.done)\n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.stack(batch.action)\n",
    "        reward_batch = torch.stack(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        with torch.no_grad():\n",
    "            next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "            next_state_values[non_final_mask] = self.target(non_final_next_states).max(1)[0].detach()\n",
    "            # Compute the expected Q values\n",
    "            expected_state_action_values = reward_batch + self.gamma * next_state_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        if self.debug:\n",
    "            self.loss_list.append(loss)\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main loop that controls the execution of the agent\"\"\"\n",
    "\n",
    "        scores = []\n",
    "        mean_scores = []\n",
    "        j = 0  # used for model2 update every c steps\n",
    "        for e in range(self.n_episodes):\n",
    "            state = self.env.reset()\n",
    "            state = torch.tensor(self.preprocess_state(state), device=self.device, dtype=torch.float)\n",
    "            done = False\n",
    "            cum_reward = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(\n",
    "                    state,\n",
    "                    self.get_epsilon(e))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = torch.tensor(self.preprocess_state(next_state), device=self.device, dtype=torch.float)\n",
    "\n",
    "                cum_reward += reward\n",
    "                self.memory.push(\n",
    "                    state,  # Converted to tensor before\n",
    "                    torch.tensor([action], device=self.device),\n",
    "                    None if done else next_state,\n",
    "                    torch.tensor(reward, device=self.device).clamp_(-1, 1),\n",
    "                    torch.tensor(not done, device=self.device, dtype=torch.bool))\n",
    "\n",
    "                if self.memory.__len__() >= self.batch_size:\n",
    "                    self.replay()\n",
    "                state = next_state\n",
    "                j += 1\n",
    "                # update second model\n",
    "                if j % self.c == 0:\n",
    "                    self.target.load_state_dict(self.model.state_dict())\n",
    "                    self.target.eval()\n",
    "\n",
    "            scores.append(cum_reward)\n",
    "            mean_score = np.mean(scores)\n",
    "            mean_scores.append(mean_score)\n",
    "            if e % 100 == 0 and self.debug:\n",
    "                print('[Episode {}] - Mean reward {}.'.format(e, mean_score))\n",
    "\n",
    "        # noinspection PyUnboundLocalVariable\n",
    "        print('[Episode {}] - Mean reward {}.'.format(e, mean_score))\n",
    "        return scores, mean_scores\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PolyDualCartPoleSolver(gym.make('CartPole-v0'), n_episodes=5000, debug=True)\n",
    "scoresPolyDual, meanPolyDual = agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se muestra el reward/score optenido por episodio\n",
    "plt.plot(np.array(scoresPolyDual), label='Dual', c='#71bc78')\n",
    "plt.plot(np.array(scoresPoly), label='Poly', c='#faa76c')\n",
    "plt.plot(np.array(scoresLineal), label='Lineal', c='#5c8cbc')\n",
    "plt.ylim(0, 200)\n",
    "plt.title('Recompensa por episodio')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(meanPolyDual), label='Dual', c='#71bc78')\n",
    "plt.plot(np.array(meanPoly), label='Poly', c='#faa76c')\n",
    "plt.plot(np.array(meanLineal), label='Lineal', c='#5c8cbc')\n",
    "plt.ylim(0, 200)\n",
    "plt.title('Recompensa promedio por episodio')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole: Aproximación con Redes Neuronales\n",
    "Como ultima mejora, vamos a reemplazar el aproximador lineal por una red neuronal. En este caso, estamos trabajando con un entorno donde directamente el agente percibe los features que representan el estado del mismo. Por lo tanto, utilizaremos una red neuronal 'fully connected' de 3 capas con la siguiente arquitectura:\n",
    "\n",
    "![nn_dense](images/nn_dense.png)\n",
    "\n",
    "En este caso, como el aproximador usado es 'no lineal', no tenemos necesidad de hacer un pre-prosesamiento de los features, como en el caso del agente 'SGD_Poly' y 'SGD_Poly_Dual', para poder capturar las relaciones no lineales entre los distintos features del entorno.\n",
    "\n",
    "### Funciones de activación:\n",
    "![activation_functions](images/activation_functions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, _input_size: int, _output_size: int, _hidden_layers: int, _hidden_size: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.input = nn.Linear(_input_size, _hidden_size)\n",
    "        self.hidden_layers = _hidden_layers\n",
    "        self.hidden = []\n",
    "        for i in range(_hidden_layers):\n",
    "            layer = nn.Linear(_hidden_size, _hidden_size)\n",
    "            self.add_module('h'+str(i), layer)\n",
    "            self.hidden.append(layer)\n",
    "        self.output = nn.Linear(_hidden_size, _output_size)\n",
    "\n",
    "        # init weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        for i in range(self.hidden_layers):\n",
    "            x = F.relu(self.hidden[i](x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env, n_episodes=3000, max_env_steps=None, gamma=0.9,\n",
    "                 epsilon=0.5, epsilon_min=0.05, epsilon_log_decay=0.001, alpha=1e-3,\n",
    "                 memory_size=10000, batch_size=256, c=10, hidden_layers=2, hidden_size=24,\n",
    "                 render=False, debug=False):\n",
    "\n",
    "        self.memory = ReplayMemory(capacity=memory_size)\n",
    "        self.env = env\n",
    "\n",
    "        # hyper-parameter setting\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.n_episodes = n_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.c = c\n",
    "        if max_env_steps is not None:\n",
    "            self.env._max_episode_steps = max_env_steps\n",
    "        self.observation_space_size = env.observation_space.shape[0]\n",
    "        self.action_space_size = env.action_space.n\n",
    "\n",
    "        self.render = render\n",
    "        self.debug = debug\n",
    "        if debug:\n",
    "            self.loss_list = []\n",
    "        # if gpu is to be used\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Init model 1\n",
    "        # Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "        # is a Module which contains other Modules, and applies them in sequence to\n",
    "        # produce its output. Each Linear Module computes output from input using a\n",
    "        # linear function, and holds internal Tensors for its weight and bias.\n",
    "        # After constructing the model we use the .to() method to move it to the\n",
    "        # desired device.\n",
    "        self.model = Net(self.observation_space_size, self.action_space_size, hidden_layers, hidden_size) \\\n",
    "            .to(self.device)\n",
    "        self.target = Net(self.observation_space_size, self.action_space_size, hidden_layers, hidden_size) \\\n",
    "            .to(self.device)\n",
    "        self.target.load_state_dict(self.model.state_dict())\n",
    "        self.target.eval()\n",
    "        self.model.train()\n",
    "\n",
    "        # The nn package also contains definitions of popular loss functions; in this\n",
    "        # case we will use Mean Squared Error (MSE) as our loss function. Setting\n",
    "        # reduction='sum' means that we are computing the *sum* of squared errors rather\n",
    "        # than the mean; this is for consistency with the examples above where we\n",
    "        # manually compute the loss, but in practice it is more common to use mean\n",
    "        # squared error as a loss by setting reduction='elementwise_mean'.\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        # Use the optim package to define an Optimizer that will update the weights of\n",
    "        # the model for us. Here we will use Adam; the optim package contains many other\n",
    "        # optimization algoriths. The first argument to the Adam constructor tells the\n",
    "        # optimizer which Tensors it should update.\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=alpha)\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        \"\"\"Chooses the next action according to the model trained and the policy\"\"\"\n",
    "\n",
    "        # exploits the current knowledge if the random number > epsilon, otherwise explores\n",
    "        if np.random.random() <= epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q = self.model(state)\n",
    "                argmax = torch.argmax(q)\n",
    "                return argmax.item()\n",
    "\n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"Returns an epsilon that decays over time until a minimum epsilon value is reached; in this case the minimum\n",
    "        value is returned\"\"\"\n",
    "        return max(self.epsilon_min, self.epsilon * math.exp(-self.epsilon_decay * episode))\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Previously stored (s, a, r, s') tuples are replayed (that is, are added into the model). The size of the\n",
    "        tuples added is determined by the batch_size parameter\"\"\"\n",
    "\n",
    "        transitions, _ = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        non_final_mask = torch.stack(batch.done)\n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.stack(batch.action)\n",
    "        reward_batch = torch.stack(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        with torch.no_grad():\n",
    "            next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "            next_state_values[non_final_mask] = self.target(non_final_next_states).max(1)[0].detach()\n",
    "            # Compute the expected Q values\n",
    "            expected_state_action_values = reward_batch + self.gamma * next_state_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        if self.debug:\n",
    "            self.loss_list.append(loss)\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main loop that controls the execution of the agent\"\"\"\n",
    "\n",
    "        scores = []\n",
    "        mean_scores = []\n",
    "        j = 0  # used for model2 update every c steps\n",
    "        for e in range(self.n_episodes):\n",
    "            state = self.env.reset()\n",
    "            state = torch.tensor(state, device=self.device, dtype=torch.float)\n",
    "            done = False\n",
    "            cum_reward = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(\n",
    "                    state,\n",
    "                    self.get_epsilon(e))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = torch.tensor(next_state, device=self.device, dtype=torch.float)\n",
    "\n",
    "                cum_reward += reward\n",
    "                self.memory.push(\n",
    "                    state,  #Converted to tensor in choose_action method\n",
    "                    torch.tensor([action], device=self.device),\n",
    "                    None if done else next_state,\n",
    "                    torch.tensor(reward, device=self.device).clamp_(-1, 1),\n",
    "                    torch.tensor(not done, device=self.device, dtype=torch.bool))\n",
    "\n",
    "                if self.memory.__len__() >= self.batch_size:\n",
    "                    self.replay()\n",
    "\n",
    "                state = next_state\n",
    "                j += 1\n",
    "\n",
    "                # update second model\n",
    "                if j % self.c == 0:\n",
    "                    self.target.load_state_dict(self.model.state_dict())\n",
    "                    self.target.eval()\n",
    "\n",
    "            scores.append(cum_reward)\n",
    "            mean_score = np.mean(scores)\n",
    "            mean_scores.append(mean_score)\n",
    "            if e % 100 == 0 and self.debug:\n",
    "                print('[Episode {}] - Mean reward {}.'.format(e, mean_score))\n",
    "\n",
    "        # noinspection PyUnboundLocalVariable\n",
    "        print('[Episode {}] - Mean reward {}.'.format(e, mean_score))\n",
    "        return scores, mean_scores\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model = torch.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN(gym.make('CartPole-v0'), n_episodes=5000, debug=True)\n",
    "scoresDQN, meanDQN = agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se muestra el reward/score optenido por episodio\n",
    "plt.plot(np.array(scoresDQN), label='DQN', c='#7e5fa4')\n",
    "plt.plot(np.array(scoresPolyDual), label='Dual', c='#71bc78')\n",
    "plt.plot(np.array(scoresPoly), label='Poly', c='#faa76c')\n",
    "plt.plot(np.array(scoresLineal), label='Lineal', c='#5c8cbc')\n",
    "plt.ylim(0, 200)\n",
    "plt.title('Recompensa por episodio')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(meanDQN), label='DQN', c='#7e5fa4')\n",
    "plt.plot(np.array(meanPolyDual), label='Dual', c='#71bc78')\n",
    "plt.plot(np.array(meanPoly), label='Poly', c='#faa76c')\n",
    "plt.plot(np.array(meanLineal), label='Lineal', c='#5c8cbc')\n",
    "plt.ylim(0, 200)\n",
    "plt.title('Recompensa promedio por episodio')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios Lab Final\n",
    "\n",
    "Se pide:\n",
    "\n",
    "1) Implementar [Double DQN (DDQN)](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12389/11847).\n",
    "\n",
    "2) Implementar [Dueling DDQN](https://arxiv.org/pdf/1511.06581.pdf).\n",
    "\n",
    "3) Cambiar el entorno por uno nuevo a elección\n",
    "\n",
    "4) Implementar [Prioritized Experience Replay (PER)](https://arxiv.org/pdf/1511.05952.pdf) (**Opcional**).\n",
    "\n",
    "**Comentar en un notebook lo realizado paso a paso, mostrando resultados parciales y finales. Y subirlo a un repositorio en GitHub** \n",
    "\n",
    "**Recomendación General**: No se sugiere hacer este TP desde jupyter notebook/lab sino desde un IDE estilo \"Pycharm\" o \"Visual Studio Code\", debido a que los algoritmos de RL suelen requerir un debug paso a paso, tanto para corregir errores como para entender mejor cómo funcionan los mismos.\n",
    "\n",
    "**Opcional**: Implementación de un agente DQN convolucional que aprende a jugar a Atari© Space Invaders© ([link](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Space%20Invaders/DQN%20Atari%20Space%20Invaders.ipynb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
